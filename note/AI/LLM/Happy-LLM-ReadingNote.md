**地址**
https://datawhalechina.github.io/happy-llm/#/./%E5%89%8D%E8%A8%80


# 前言
2022年底，ChatGPT 的横空出世改变了人们对人工智能的认知，也给**自然语言处理（Natural Language Process，NLP）领域**带来了阶段性的变革，以 GPT 系列模型为代表的**大语言模型（Large Language Model，LLM**成为 NLP 乃至人工智能领域的研究主流。自 2023年至今，LLM 始终是人工智能领域的核心话题，引发了一轮又一轮的科技浪潮。

LLM 其实是 NLP 领域经典研究方法预训练语言模型（Pretrain Language Model，PLM）的一种衍生成果。NLP 领域聚焦于人类书写的自然语言文本的处理、理解和生成，从诞生至今经历了符号主义阶段、统计学习阶段、深度学习阶段、预训练模型阶段到而今大模型阶段的多次变革。
以 GPT、BERT 为代表的 PLM 是上一阶段 NLP 领域的核心研究成果，**以注意力机制为模型架构，通过预训练-微调的阶段思想通过在海量无监督文本上进行自监督预训练**，实现了强大的自然语言理解能力。
LLM 是在 PLM 的基础上，通过大量**扩大模型参数、预训练数据规模，并引入指令微调、人类反馈强化学**习等手段实现的突破性成果

---
我们在 2023年底分别创建了 self-llm（开源大模型食用指南：https://github.com/datawhalechina/self-llm ）、llm-universe（动手学大模型应用开发：https://github.com/datawhalechina/llm-universe ）两个原创开源大模型教程，前者旨在**为开发者提供一站式开源 LLM 部署、推理、微调的使用教程，后者旨在指导开发者从零开始搭建自己的 LLM 应用**


--------

# 第一章 NLP 基础概念



